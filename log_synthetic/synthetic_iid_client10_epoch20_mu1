Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_iid
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 1.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/5.44k flops)
  dense/kernel/Regularizer/l2_regularizer (1/1.80k flops)
    dense/kernel/Regularizer/l2_regularizer/L2Loss (1.80k/1.80k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/AssignSub (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/AssignSub (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.10683229813664596
At round 0 training accuracy: 0.09462123345536469
At round 0 training loss: 2.3289744154697267
gradient difference: 0.043571704441976375
At round 1 accuracy: 0.2496894409937888
At round 1 training accuracy: 0.25781470008448326
At round 1 training loss: 2.1444273597923544
gradient difference: 0.04169583280052644
At round 2 accuracy: 0.40372670807453415
At round 2 training accuracy: 0.43170937764010137
At round 2 training loss: 1.9953253030508413
gradient difference: 0.03981661851906569
At round 3 accuracy: 0.49440993788819876
At round 3 training accuracy: 0.504505773021684
At round 3 training loss: 1.857432598061912
gradient difference: 0.03775687362650693
At round 4 accuracy: 0.506832298136646
At round 4 training accuracy: 0.5250633624331175
At round 4 training loss: 1.7596727773962608
gradient difference: 0.03610712445008438
At round 5 accuracy: 0.5167701863354037
At round 5 training accuracy: 0.5313996057448606
At round 5 training loss: 1.6744112579193426
gradient difference: 0.03460449829649152
At round 6 accuracy: 0.5254658385093167
At round 6 training accuracy: 0.5364686003942551
At round 6 training loss: 1.603706928566455
gradient difference: 0.03332351415629637
At round 7 accuracy: 0.5254658385093167
At round 7 training accuracy: 0.5401295409743734
At round 7 training loss: 1.5508310217751278
gradient difference: 0.032372869299796776
At round 8 accuracy: 0.5329192546583851
At round 8 training accuracy: 0.5470290059138271
At round 8 training loss: 1.5068353194480075
gradient difference: 0.03163342755290877
At round 9 accuracy: 0.5354037267080746
At round 9 training accuracy: 0.5487186707969586
At round 9 training loss: 1.4701359777408933
gradient difference: 0.031002249411216137
At round 10 accuracy: 0.5403726708074534
At round 10 training accuracy: 0.5522388059701493
At round 10 training loss: 1.4394949520013596
gradient difference: 0.03050086803002145
At round 11 accuracy: 0.5403726708074534
At round 11 training accuracy: 0.5563221627710504
At round 11 training loss: 1.4121112322109044
gradient difference: 0.030041514614768682
At round 12 accuracy: 0.546583850931677
At round 12 training accuracy: 0.5616727682343002
At round 12 training loss: 1.388846489232347
gradient difference: 0.029684223387235763
At round 13 accuracy: 0.5478260869565217
At round 13 training accuracy: 0.5639256547451422
At round 13 training loss: 1.3689760497777235
gradient difference: 0.02937944196665458
At round 14 accuracy: 0.5515527950310559
At round 14 training accuracy: 0.5664601520698395
At round 14 training loss: 1.3512468565957239
gradient difference: 0.029102891297635845
At round 15 accuracy: 0.5602484472049689
At round 15 training accuracy: 0.5698394818361026
At round 15 training loss: 1.335864677660165
gradient difference: 0.02886704358033643
At round 16 accuracy: 0.5614906832298137
At round 16 training accuracy: 0.572655589974655
At round 16 training loss: 1.3209541546093246
gradient difference: 0.028630310642595847
At round 17 accuracy: 0.5614906832298137
At round 17 training accuracy: 0.574908476485497
At round 17 training loss: 1.3065615145296017
gradient difference: 0.028413027942689566
At round 18 accuracy: 0.5627329192546584
At round 18 training accuracy: 0.5794142495071811
At round 18 training loss: 1.2935487729366382
gradient difference: 0.028206524973765502
At round 19 accuracy: 0.5639751552795031
At round 19 training accuracy: 0.5823711630526612
At round 19 training loss: 1.2811428397449094
gradient difference: 0.02798803234502938
At round 20 accuracy: 0.568944099378882
At round 20 training accuracy: 0.5850464657842861
At round 20 training loss: 1.2699974933264122
gradient difference: 0.02780681765186676
At round 21 accuracy: 0.5714285714285714
At round 21 training accuracy: 0.5872993522951281
At round 21 training loss: 1.2590836475043858
gradient difference: 0.027617678466620922
At round 22 accuracy: 0.5739130434782609
At round 22 training accuracy: 0.5901154604336807
At round 22 training loss: 1.2490224358196897
gradient difference: 0.02746528293060159
At round 23 accuracy: 0.5788819875776398
At round 23 training accuracy: 0.5927907631653055
At round 23 training loss: 1.2399891309421252
gradient difference: 0.027289325984724756
At round 24 accuracy: 0.5801242236024845
At round 24 training accuracy: 0.5957476767107857
At round 24 training loss: 1.2316294734795574
gradient difference: 0.027142165377169195
At round 25 accuracy: 0.5801242236024845
At round 25 training accuracy: 0.5987045902562659
At round 25 training loss: 1.222541008439409
gradient difference: 0.027002664078163107
At round 26 accuracy: 0.591304347826087
At round 26 training accuracy: 0.601661503801746
At round 26 training loss: 1.2137891957203257
gradient difference: 0.026874361623631163
At round 27 accuracy: 0.591304347826087
At round 27 training accuracy: 0.6023655308363841
At round 27 training loss: 1.2055229064915356
gradient difference: 0.02673422593495981
At round 28 accuracy: 0.5925465838509317
At round 28 training accuracy: 0.6049000281610813
At round 28 training loss: 1.197507631338942
gradient difference: 0.02661147843894756
At round 29 accuracy: 0.5950310559006211
At round 29 training accuracy: 0.6074345254857787
At round 29 training loss: 1.1899137285117598
gradient difference: 0.026486004311252698
At round 30 accuracy: 0.6
At round 30 training accuracy: 0.6096874119966207
At round 30 training loss: 1.1827710612630615
gradient difference: 0.02634844365850305
At round 31 accuracy: 0.6024844720496895
At round 31 training accuracy: 0.6115178822866798
At round 31 training loss: 1.1760011835690451
gradient difference: 0.02623180614957026
At round 32 accuracy: 0.6049689440993788
At round 32 training accuracy: 0.612925936355956
At round 32 training loss: 1.1695877083990815
gradient difference: 0.02612826037658476
At round 33 accuracy: 0.6062111801242236
At round 33 training accuracy: 0.6148972120529428
At round 33 training loss: 1.1626939782287933
gradient difference: 0.025995965153858325
At round 34 accuracy: 0.6074534161490683
At round 34 training accuracy: 0.6156012390875809
At round 34 training loss: 1.1569476294134813
gradient difference: 0.025886915707876094
At round 35 accuracy: 0.6086956521739131
At round 35 training accuracy: 0.617854125598423
At round 35 training loss: 1.150572183181856
gradient difference: 0.025763182491050193
At round 36 accuracy: 0.6074534161490683
At round 36 training accuracy: 0.6196845958884821
At round 36 training loss: 1.1445497432684302
gradient difference: 0.02565923206191212
At round 37 accuracy: 0.6074534161490683
At round 37 training accuracy: 0.6220782878062517
At round 37 training loss: 1.1390089216416266
gradient difference: 0.025544781602148074
At round 38 accuracy: 0.6086956521739131
At round 38 training accuracy: 0.6240495635032385
At round 38 training loss: 1.1332778767510825
gradient difference: 0.02544758417950673
At round 39 accuracy: 0.6124223602484472
At round 39 training accuracy: 0.626161644607153
At round 39 training loss: 1.1275095335700687
gradient difference: 0.025340370560082644
At round 40 accuracy: 0.613664596273292
At round 40 training accuracy: 0.6265840608279358
At round 40 training loss: 1.1224125710483068
gradient difference: 0.025249966474835905
At round 41 accuracy: 0.6173913043478261
At round 41 training accuracy: 0.628414531117995
At round 41 training loss: 1.1175249162542085
gradient difference: 0.025129610619825323
At round 42 accuracy: 0.622360248447205
At round 42 training accuracy: 0.6296817797803436
At round 42 training loss: 1.1125455633481434
gradient difference: 0.025033726297194567
At round 43 accuracy: 0.6260869565217392
At round 43 training accuracy: 0.6310898338496198
At round 43 training loss: 1.1073468651646663
gradient difference: 0.024926664388999598
At round 44 accuracy: 0.6260869565217392
At round 44 training accuracy: 0.6330611095466065
At round 44 training loss: 1.1021464845725295
gradient difference: 0.02483958831556301
At round 45 accuracy: 0.6285714285714286
At round 45 training accuracy: 0.6350323852435934
At round 45 training loss: 1.0971304109418134
gradient difference: 0.02473797025190218
At round 46 accuracy: 0.631055900621118
At round 46 training accuracy: 0.6358772176851591
At round 46 training loss: 1.0924781849269496
gradient difference: 0.024675991598631437
At round 47 accuracy: 0.6298136645962733
At round 47 training accuracy: 0.6371444663475078
At round 47 training loss: 1.0877562367691185
gradient difference: 0.024579754092950983
At round 48 accuracy: 0.6298136645962733
At round 48 training accuracy: 0.6402421852999155
At round 48 training loss: 1.0831220077481347
gradient difference: 0.02447069578044362
At round 49 accuracy: 0.6285714285714286
At round 49 training accuracy: 0.6412278231484089
At round 49 training loss: 1.078965616105342
gradient difference: 0.024381753394170484
At round 50 accuracy: 0.6285714285714286
At round 50 training accuracy: 0.6436215150661786
At round 50 training loss: 1.0745842945605928
gradient difference: 0.02432210833143598
At round 51 accuracy: 0.631055900621118
At round 51 training accuracy: 0.6444663475077443
At round 51 training loss: 1.0706393330223425
gradient difference: 0.024272815926080122
At round 52 accuracy: 0.6322981366459627
At round 52 training accuracy: 0.645733596170093
At round 52 training loss: 1.0665556535322007
gradient difference: 0.024180144928994098
At round 53 accuracy: 0.6335403726708074
At round 53 training accuracy: 0.6464376232047311
At round 53 training loss: 1.062328182116793
gradient difference: 0.024098195458350003
At round 54 accuracy: 0.6360248447204969
At round 54 training accuracy: 0.6471416502393692
At round 54 training loss: 1.0589049599080715
gradient difference: 0.02401520110303655
At round 55 accuracy: 0.6360248447204969
At round 55 training accuracy: 0.6482680934947902
At round 55 training loss: 1.0548001610957478
gradient difference: 0.023938490050127208
At round 56 accuracy: 0.6385093167701863
At round 56 training accuracy: 0.6499577583779217
At round 56 training loss: 1.0509799488814104
gradient difference: 0.02387977551656084
At round 57 accuracy: 0.639751552795031
At round 57 training accuracy: 0.6512250070402703
At round 57 training loss: 1.0475394957352946
gradient difference: 0.02380861423450018
At round 58 accuracy: 0.6409937888198758
At round 58 training accuracy: 0.6516474232610532
At round 58 training loss: 1.0436500275816525
gradient difference: 0.023731538314903884
At round 59 accuracy: 0.639751552795031
At round 59 training accuracy: 0.6530554773303295
At round 59 training loss: 1.040146159776598
gradient difference: 0.023680507759752233
At round 60 accuracy: 0.6422360248447205
At round 60 training accuracy: 0.6543227259926782
At round 60 training loss: 1.0367107359801033
gradient difference: 0.023610066285207065
At round 61 accuracy: 0.6422360248447205
At round 61 training accuracy: 0.6529146719234019
At round 61 training loss: 1.0333219562171714
gradient difference: 0.023572180170168027
At round 62 accuracy: 0.6409937888198758
At round 62 training accuracy: 0.6543227259926782
At round 62 training loss: 1.02992857324174
gradient difference: 0.023529654242823623
At round 63 accuracy: 0.6434782608695652
At round 63 training accuracy: 0.6562940016896649
At round 63 training loss: 1.0262413299090425
gradient difference: 0.02343931940638875
At round 64 accuracy: 0.6434782608695652
At round 64 training accuracy: 0.658124471979724
At round 64 training loss: 1.0229414121277465
gradient difference: 0.023370136700414098
At round 65 accuracy: 0.6447204968944099
At round 65 training accuracy: 0.6588284990143621
At round 65 training loss: 1.0194823413759981
gradient difference: 0.02329052159557997
At round 66 accuracy: 0.6459627329192547
At round 66 training accuracy: 0.6619262179667699
At round 66 training loss: 1.0166391621204
gradient difference: 0.023206047205513978
At round 67 accuracy: 0.6484472049689441
At round 67 training accuracy: 0.663052661222191
At round 67 training loss: 1.0132509576069137
gradient difference: 0.02313084547308408
At round 68 accuracy: 0.6521739130434783
At round 68 training accuracy: 0.6633342720360462
At round 68 training loss: 1.0100428406133413
gradient difference: 0.02306359126376487
At round 69 accuracy: 0.653416149068323
At round 69 training accuracy: 0.6658687693607435
At round 69 training loss: 1.00709491355491
gradient difference: 0.022964118389664667
At round 70 accuracy: 0.6546583850931676
At round 70 training accuracy: 0.667558434243875
At round 70 training loss: 1.0040087341523714
gradient difference: 0.02290412212348106
At round 71 accuracy: 0.6559006211180124
At round 71 training accuracy: 0.6689664883131512
At round 71 training loss: 1.0008450999239136
gradient difference: 0.022838625362611
At round 72 accuracy: 0.6546583850931676
At round 72 training accuracy: 0.6696705153477893
At round 72 training loss: 0.9978381249979159
gradient difference: 0.022779403835629798
At round 73 accuracy: 0.6559006211180124
At round 73 training accuracy: 0.6692480991270064
At round 73 training loss: 0.9948138746534458
gradient difference: 0.022736753433310195
At round 74 accuracy: 0.6559006211180124
At round 74 training accuracy: 0.6700929315685722
At round 74 training loss: 0.9918003469680404
gradient difference: 0.022682553255447397
At round 75 accuracy: 0.6559006211180124
At round 75 training accuracy: 0.6710785694170656
At round 75 training loss: 0.988765601220852
gradient difference: 0.02261985176994474
At round 76 accuracy: 0.6583850931677019
At round 76 training accuracy: 0.6719234018586314
At round 76 training loss: 0.9858697450385904
gradient difference: 0.02256735864722344
At round 77 accuracy: 0.6559006211180124
At round 77 training accuracy: 0.6727682343001972
At round 77 training loss: 0.9830318941545634
gradient difference: 0.02252745549190781
At round 78 accuracy: 0.6571428571428571
At round 78 training accuracy: 0.6737538721486905
At round 78 training loss: 0.9802844267568264
gradient difference: 0.022477398987777793
At round 79 accuracy: 0.6596273291925466
At round 79 training accuracy: 0.6740354829625458
At round 79 training loss: 0.9779819855972667
gradient difference: 0.022402863670731965
At round 80 accuracy: 0.6596273291925466
At round 80 training accuracy: 0.6744578991833287
At round 80 training loss: 0.9751735470204983
gradient difference: 0.022344327447976525
At round 81 accuracy: 0.6596273291925466
At round 81 training accuracy: 0.6757251478456773
At round 81 training loss: 0.9724583121059109
gradient difference: 0.022291412397576427
At round 82 accuracy: 0.6596273291925466
At round 82 training accuracy: 0.6764291748803154
At round 82 training loss: 0.9696735097837595
gradient difference: 0.022231778764409117
At round 83 accuracy: 0.6608695652173913
At round 83 training accuracy: 0.6771332019149535
At round 83 training loss: 0.9669333258940112
gradient difference: 0.022171583810427893
At round 84 accuracy: 0.662111801242236
At round 84 training accuracy: 0.6769923965080259
At round 84 training loss: 0.9643233199248479
gradient difference: 0.022115486150003846
At round 85 accuracy: 0.6645962732919255
At round 85 training accuracy: 0.6778372289495916
At round 85 training loss: 0.9617757673113825
gradient difference: 0.022063807920942067
At round 86 accuracy: 0.6645962732919255
At round 86 training accuracy: 0.6792452830188679
At round 86 training loss: 0.9595529891275681
gradient difference: 0.022023115894175403
At round 87 accuracy: 0.6658385093167701
At round 87 training accuracy: 0.6795268938327231
At round 87 training loss: 0.9570192935952869
gradient difference: 0.021981196925176954
At round 88 accuracy: 0.6645962732919255
At round 88 training accuracy: 0.6802309208673613
At round 88 training loss: 0.9545515798340714
gradient difference: 0.021916975825796388
At round 89 accuracy: 0.6658385093167701
At round 89 training accuracy: 0.6819205857504929
At round 89 training loss: 0.9521735370058437
gradient difference: 0.02185775263831499
At round 90 accuracy: 0.6658385093167701
At round 90 training accuracy: 0.6823430019712757
At round 90 training loss: 0.9496624355365175
gradient difference: 0.021810003057237286
At round 91 accuracy: 0.6645962732919255
At round 91 training accuracy: 0.6831878344128415
At round 91 training loss: 0.9471773379404959
gradient difference: 0.021756835608245966
At round 92 accuracy: 0.6658385093167701
At round 92 training accuracy: 0.6833286398197691
At round 92 training loss: 0.9447734869557077
gradient difference: 0.021727700957272885
At round 93 accuracy: 0.6670807453416149
At round 93 training accuracy: 0.6834694452266967
At round 93 training loss: 0.9423551140151135
gradient difference: 0.021669777082472474
At round 94 accuracy: 0.6670807453416149
At round 94 training accuracy: 0.6831878344128415
At round 94 training loss: 0.9400302726377806
gradient difference: 0.02164235917299738
At round 95 accuracy: 0.6645962732919255
At round 95 training accuracy: 0.6845958884821177
At round 95 training loss: 0.9378923365462837
gradient difference: 0.02160337434038889
At round 96 accuracy: 0.6658385093167701
At round 96 training accuracy: 0.6851591101098282
At round 96 training loss: 0.9355638720340643
gradient difference: 0.0215468915801329
At round 97 accuracy: 0.6658385093167701
At round 97 training accuracy: 0.6857223317375387
At round 97 training loss: 0.9335909084731913
gradient difference: 0.021534899080972607
At round 98 accuracy: 0.6658385093167701
At round 98 training accuracy: 0.6867079695860321
At round 98 training loss: 0.9314105281592154
gradient difference: 0.021484138629663616
At round 99 accuracy: 0.6683229813664596
At round 99 training accuracy: 0.6869895803998873
At round 99 training loss: 0.929234468418254
gradient difference: 0.021432795127856185
At round 100 accuracy: 0.6683229813664596
At round 100 training accuracy: 0.6872711912137426
At round 100 training loss: 0.9270038926342916
gradient difference: 0.02138638430516942
At round 101 accuracy: 0.6658385093167701
At round 101 training accuracy: 0.6881160236553083
At round 101 training loss: 0.924804407353134
gradient difference: 0.021334199773086576
At round 102 accuracy: 0.6695652173913044
At round 102 training accuracy: 0.6886792452830188
At round 102 training loss: 0.9227615625245307
gradient difference: 0.021298614677019097
At round 103 accuracy: 0.6708074534161491
At round 103 training accuracy: 0.6886792452830188
At round 103 training loss: 0.920662008777735
gradient difference: 0.02124461650343933
At round 104 accuracy: 0.6670807453416149
At round 104 training accuracy: 0.689383272317657
At round 104 training loss: 0.9190672876481707
gradient difference: 0.021174035113299742
At round 105 accuracy: 0.6695652173913044
At round 105 training accuracy: 0.6898056885384398
At round 105 training loss: 0.9169219184244092
gradient difference: 0.021141156382823136
At round 106 accuracy: 0.6720496894409937
At round 106 training accuracy: 0.6895240777245846
At round 106 training loss: 0.9148360066550176
gradient difference: 0.021099303465856374
At round 107 accuracy: 0.6708074534161491
At round 107 training accuracy: 0.6900872993522952
At round 107 training loss: 0.9129401427596363
gradient difference: 0.021067944481823867
At round 108 accuracy: 0.6745341614906832
At round 108 training accuracy: 0.689383272317657
At round 108 training loss: 0.9109443101588185
gradient difference: 0.021030515980683864
At round 109 accuracy: 0.6732919254658385
At round 109 training accuracy: 0.690509715573078
At round 109 training loss: 0.9092342416204825
gradient difference: 0.021000824141697304
At round 110 accuracy: 0.6720496894409937
At round 110 training accuracy: 0.690509715573078
At round 110 training loss: 0.9072044624371852
gradient difference: 0.020952299997341103
At round 111 accuracy: 0.6745341614906832
At round 111 training accuracy: 0.6919177696423543
At round 111 training loss: 0.9051844412358637
gradient difference: 0.020890381504849086
At round 112 accuracy: 0.675776397515528
At round 112 training accuracy: 0.6929034074908477
At round 112 training loss: 0.9032258781387047
gradient difference: 0.020844803703349735
At round 113 accuracy: 0.6770186335403726
At round 113 training accuracy: 0.6943114615601239
At round 113 training loss: 0.9014324451191128
gradient difference: 0.020810739830574396
At round 114 accuracy: 0.675776397515528
At round 114 training accuracy: 0.6948746831878344
At round 114 training loss: 0.8996784249952698
gradient difference: 0.020759757334719443
At round 115 accuracy: 0.6745341614906832
At round 115 training accuracy: 0.6948746831878344
At round 115 training loss: 0.8977932951393949
gradient difference: 0.02073137211798303
At round 116 accuracy: 0.675776397515528
At round 116 training accuracy: 0.6951562940016897
At round 116 training loss: 0.8961086207898493
gradient difference: 0.020710953887419798
At round 117 accuracy: 0.675776397515528
At round 117 training accuracy: 0.6957195156294002
At round 117 training loss: 0.894343725697016
gradient difference: 0.020665186130881932
At round 118 accuracy: 0.675776397515528
At round 118 training accuracy: 0.6964235426640383
At round 118 training loss: 0.8924559520117433
gradient difference: 0.020630415064593236
At round 119 accuracy: 0.6770186335403726
At round 119 training accuracy: 0.6965643480709659
At round 119 training loss: 0.8905926864353981
gradient difference: 0.02058678577471393
At round 120 accuracy: 0.6795031055900621
At round 120 training accuracy: 0.697268375105604
At round 120 training loss: 0.8889680471144338
gradient difference: 0.020551653854842977
At round 121 accuracy: 0.6782608695652174
At round 121 training accuracy: 0.697268375105604
At round 121 training loss: 0.8874301358499315
gradient difference: 0.0205299608565087
At round 122 accuracy: 0.6795031055900621
At round 122 training accuracy: 0.6974091805125316
At round 122 training loss: 0.885570356381574
gradient difference: 0.020497233924805177
At round 123 accuracy: 0.6795031055900621
At round 123 training accuracy: 0.6985356237679526
At round 123 training loss: 0.883785115819822
gradient difference: 0.020460235917480744
At round 124 accuracy: 0.6795031055900621
At round 124 training accuracy: 0.6989580399887355
At round 124 training loss: 0.8819610979549651
gradient difference: 0.020416244060883767
At round 125 accuracy: 0.6819875776397516
At round 125 training accuracy: 0.6996620670233737
At round 125 training loss: 0.8802583506915711
gradient difference: 0.02036704909696864
At round 126 accuracy: 0.6832298136645962
At round 126 training accuracy: 0.7002252886510842
At round 126 training loss: 0.8786085680467046
gradient difference: 0.02034326790739485
At round 127 accuracy: 0.684472049689441
At round 127 training accuracy: 0.7006477048718671
At round 127 training loss: 0.8768639204720986
gradient difference: 0.020307345920571165
At round 128 accuracy: 0.6857142857142857
At round 128 training accuracy: 0.7012109264995776
At round 128 training loss: 0.8751496800366538
gradient difference: 0.020265533880229565
At round 129 accuracy: 0.6857142857142857
At round 129 training accuracy: 0.7017741481272881
At round 129 training loss: 0.8734895608033035
gradient difference: 0.020230942020723012
At round 130 accuracy: 0.6857142857142857
At round 130 training accuracy: 0.7017741481272881
At round 130 training loss: 0.8718633891763368
gradient difference: 0.020192445251241073
At round 131 accuracy: 0.6857142857142857
At round 131 training accuracy: 0.7027597859757815
At round 131 training loss: 0.8702302793991864
gradient difference: 0.020164525201070416
At round 132 accuracy: 0.6869565217391305
At round 132 training accuracy: 0.703323007603492
At round 132 training loss: 0.8685916604978271
gradient difference: 0.020112334563582537
At round 133 accuracy: 0.6881987577639752
At round 133 training accuracy: 0.7038862292312025
At round 133 training loss: 0.8668915387258769
gradient difference: 0.02007853783152329
At round 134 accuracy: 0.6919254658385093
At round 134 training accuracy: 0.7048718670796958
At round 134 training loss: 0.8654060374260352
gradient difference: 0.020069386911976076
At round 135 accuracy: 0.6919254658385093
At round 135 training accuracy: 0.7057166995212616
At round 135 training loss: 0.863949953511009
gradient difference: 0.020064311093572314
At round 136 accuracy: 0.6931677018633541
At round 136 training accuracy: 0.7062799211489721
At round 136 training loss: 0.8623959933056424
gradient difference: 0.02002445793001949
At round 137 accuracy: 0.6931677018633541
At round 137 training accuracy: 0.7065615319628273
At round 137 training loss: 0.8609011771896462
gradient difference: 0.019990822010559607
At round 138 accuracy: 0.6931677018633541
At round 138 training accuracy: 0.7064207265558997
At round 138 training loss: 0.8595584531848917
gradient difference: 0.019959758876320132
At round 139 accuracy: 0.6931677018633541
At round 139 training accuracy: 0.7075471698113207
At round 139 training loss: 0.8579231916481728
gradient difference: 0.01990053964994566
At round 140 accuracy: 0.6931677018633541
At round 140 training accuracy: 0.7074063644043931
At round 140 training loss: 0.8564520786426935
gradient difference: 0.019880045363231895
At round 141 accuracy: 0.6931677018633541
At round 141 training accuracy: 0.7082511968459588
At round 141 training loss: 0.8550523619556454
gradient difference: 0.019868019123171952
At round 142 accuracy: 0.6931677018633541
At round 142 training accuracy: 0.7086736130667418
At round 142 training loss: 0.8536477152460631
gradient difference: 0.01985314145843199
At round 143 accuracy: 0.6931677018633541
At round 143 training accuracy: 0.7090960292875247
At round 143 training loss: 0.852144066656653
gradient difference: 0.019825145359989544
At round 144 accuracy: 0.6919254658385093
At round 144 training accuracy: 0.7096592509152352
At round 144 training loss: 0.8505533685552608
gradient difference: 0.019773881071567658
At round 145 accuracy: 0.6931677018633541
At round 145 training accuracy: 0.7103632779498733
At round 145 training loss: 0.849107052324121
gradient difference: 0.019750913238699666
At round 146 accuracy: 0.6931677018633541
At round 146 training accuracy: 0.7107856941706562
At round 146 training loss: 0.8475540901398666
gradient difference: 0.01971157151793079
At round 147 accuracy: 0.6919254658385093
At round 147 training accuracy: 0.7120529428330048
At round 147 training loss: 0.8461719693784411
gradient difference: 0.019669255708541087
At round 148 accuracy: 0.6931677018633541
At round 148 training accuracy: 0.7143058293438468
At round 148 training loss: 0.8446049974086217
gradient difference: 0.019608028731133143
At round 149 accuracy: 0.6944099378881987
At round 149 training accuracy: 0.7159954942269783
At round 149 training loss: 0.8431086596872598
gradient difference: 0.01957072885943627
At round 150 accuracy: 0.6931677018633541
At round 150 training accuracy: 0.7169811320754716
At round 150 training loss: 0.8416565973400835
gradient difference: 0.019525021291921938
At round 151 accuracy: 0.6944099378881987
At round 151 training accuracy: 0.716840326668544
At round 151 training loss: 0.8402497351958496
gradient difference: 0.019490956838914976
At round 152 accuracy: 0.6968944099378882
At round 152 training accuracy: 0.7172627428893269
At round 152 training loss: 0.8388722905455358
gradient difference: 0.019462086815931635
At round 153 accuracy: 0.698136645962733
At round 153 training accuracy: 0.7185299915516756
At round 153 training loss: 0.8376142692059002
gradient difference: 0.01944680184739167
At round 154 accuracy: 0.6956521739130435
At round 154 training accuracy: 0.718389186144748
At round 154 training loss: 0.8362739215817663
gradient difference: 0.01943781851113664
At round 155 accuracy: 0.6968944099378882
At round 155 training accuracy: 0.7186707969586033
At round 155 training loss: 0.8348292822620963
gradient difference: 0.019401232292736933
At round 156 accuracy: 0.6968944099378882
At round 156 training accuracy: 0.7193748239932414
At round 156 training loss: 0.8334323697545366
gradient difference: 0.019371115257708946
At round 157 accuracy: 0.6968944099378882
At round 157 training accuracy: 0.7193748239932414
At round 157 training loss: 0.8320472965489573
gradient difference: 0.01935287979402213
At round 158 accuracy: 0.6968944099378882
At round 158 training accuracy: 0.7197972402140242
At round 158 training loss: 0.8307080904016089
gradient difference: 0.019339202595677608
At round 159 accuracy: 0.6968944099378882
At round 159 training accuracy: 0.7205012672486624
At round 159 training loss: 0.8293213950214638
gradient difference: 0.01930532007470644
At round 160 accuracy: 0.6956521739130435
At round 160 training accuracy: 0.72064207265559
At round 160 training loss: 0.8281332242408291
gradient difference: 0.019261057792854384
At round 161 accuracy: 0.6968944099378882
At round 161 training accuracy: 0.7202196564348071
At round 161 training loss: 0.8268356307715773
gradient difference: 0.019248959038442186
At round 162 accuracy: 0.6968944099378882
At round 162 training accuracy: 0.72064207265559
At round 162 training loss: 0.8254920896326311
gradient difference: 0.01922258076061986
At round 163 accuracy: 0.6968944099378882
At round 163 training accuracy: 0.7213460996902281
At round 163 training loss: 0.824136030074745
gradient difference: 0.019184008104831642
At round 164 accuracy: 0.6968944099378882
At round 164 training accuracy: 0.721768515911011
At round 164 training loss: 0.822891406469431
gradient difference: 0.0191717611667905
At round 165 accuracy: 0.6956521739130435
At round 165 training accuracy: 0.7221909321317939
At round 165 training loss: 0.8217753697552838
gradient difference: 0.019121234864758276
At round 166 accuracy: 0.6968944099378882
At round 166 training accuracy: 0.7230357645733596
At round 166 training loss: 0.8204320662711849
gradient difference: 0.019088682583581082
At round 167 accuracy: 0.6968944099378882
At round 167 training accuracy: 0.7231765699802872
At round 167 training loss: 0.819296612576074
gradient difference: 0.019047634155976545
At round 168 accuracy: 0.6968944099378882
At round 168 training accuracy: 0.7237397916079977
At round 168 training loss: 0.8182362061895071
gradient difference: 0.019025146568079257
At round 169 accuracy: 0.7006211180124223
At round 169 training accuracy: 0.7238805970149254
At round 169 training loss: 0.8172265973704796
gradient difference: 0.018995756023023724
At round 170 accuracy: 0.7031055900621118
At round 170 training accuracy: 0.7244438186426359
At round 170 training loss: 0.8159842989002272
gradient difference: 0.018961310584064302
At round 171 accuracy: 0.7018633540372671
At round 171 training accuracy: 0.7243030132357082
At round 171 training loss: 0.8147805876310226
gradient difference: 0.018957413644427677
At round 172 accuracy: 0.6993788819875777
At round 172 training accuracy: 0.7235989862010701
At round 172 training loss: 0.8135097422119061
gradient difference: 0.0189333877864021
At round 173 accuracy: 0.7006211180124223
At round 173 training accuracy: 0.7241622078287806
At round 173 training loss: 0.8123716948503039
gradient difference: 0.01890624339456466
At round 174 accuracy: 0.7006211180124223
At round 174 training accuracy: 0.7247254294564911
At round 174 training loss: 0.8111280548559112
gradient difference: 0.01890305605508745
At round 175 accuracy: 0.7006211180124223
At round 175 training accuracy: 0.7245846240495635
At round 175 training loss: 0.8098505389452048
gradient difference: 0.018866250151874365
At round 176 accuracy: 0.7006211180124223
At round 176 training accuracy: 0.7257110673049845
At round 176 training loss: 0.8085787218914875
gradient difference: 0.018817371761439827
At round 177 accuracy: 0.7006211180124223
At round 177 training accuracy: 0.726274288932695
At round 177 training loss: 0.8073673108301509
gradient difference: 0.018800394396217245
At round 178 accuracy: 0.7006211180124223
At round 178 training accuracy: 0.7279639538158266
At round 178 training loss: 0.8063511901743075
gradient difference: 0.018778055941633478
At round 179 accuracy: 0.7018633540372671
At round 179 training accuracy: 0.7281047592227542
At round 179 training loss: 0.8053095830859885
gradient difference: 0.018757087414019106
At round 180 accuracy: 0.7018633540372671
At round 180 training accuracy: 0.7283863700366094
At round 180 training loss: 0.8042462464538436
gradient difference: 0.01872745189080082
At round 181 accuracy: 0.7018633540372671
At round 181 training accuracy: 0.7295128132920304
At round 181 training loss: 0.8032583080859494
gradient difference: 0.018721534007308884
At round 182 accuracy: 0.7031055900621118
At round 182 training accuracy: 0.7300760349197409
At round 182 training loss: 0.8020477902298409
gradient difference: 0.018687295198053466
At round 183 accuracy: 0.7018633540372671
At round 183 training accuracy: 0.7303576457335962
At round 183 training loss: 0.8007994939058272
gradient difference: 0.01865551402990307
At round 184 accuracy: 0.7031055900621118
At round 184 training accuracy: 0.7296536186989581
At round 184 training loss: 0.7996367630371742
gradient difference: 0.018645285004915836
At round 185 accuracy: 0.7018633540372671
At round 185 training accuracy: 0.7299352295128133
At round 185 training loss: 0.7984441099318609
gradient difference: 0.01862406223105687
At round 186 accuracy: 0.7068322981366459
At round 186 training accuracy: 0.7306392565474514
At round 186 training loss: 0.7972397084841759
gradient difference: 0.01860148721453808
At round 187 accuracy: 0.7031055900621118
At round 187 training accuracy: 0.7304984511405238
At round 187 training loss: 0.7961053577603437
gradient difference: 0.018557511221904348
At round 188 accuracy: 0.7031055900621118
At round 188 training accuracy: 0.7314840889890172
At round 188 training loss: 0.794914153479281
gradient difference: 0.018522968677857767
At round 189 accuracy: 0.7068322981366459
At round 189 training accuracy: 0.7319065052098
At round 189 training loss: 0.7937418585829384
gradient difference: 0.0184856682473785
At round 190 accuracy: 0.7105590062111802
At round 190 training accuracy: 0.7326105322444382
At round 190 training loss: 0.7927349692698903
gradient difference: 0.018491563103281435
At round 191 accuracy: 0.7105590062111802
At round 191 training accuracy: 0.7328921430582934
At round 191 training loss: 0.7917733147794916
gradient difference: 0.018478847969180697
At round 192 accuracy: 0.7118012422360248
At round 192 training accuracy: 0.7335961700929315
At round 192 training loss: 0.7907180847986689
gradient difference: 0.018481919031724112
At round 193 accuracy: 0.7130434782608696
At round 193 training accuracy: 0.7343001971275697
At round 193 training loss: 0.7895793316986016
gradient difference: 0.018435790441135875
At round 194 accuracy: 0.7105590062111802
At round 194 training accuracy: 0.7344410025344973
At round 194 training loss: 0.7884816711081883
gradient difference: 0.018396840483580423
At round 195 accuracy: 0.7105590062111802
At round 195 training accuracy: 0.7345818079414249
At round 195 training loss: 0.787349112904666
gradient difference: 0.018373006680689023
At round 196 accuracy: 0.7093167701863354
At round 196 training accuracy: 0.7337369754998592
At round 196 training loss: 0.7862766220582368
gradient difference: 0.018340818408305166
At round 197 accuracy: 0.7080745341614907
At round 197 training accuracy: 0.7348634187552802
At round 197 training loss: 0.7852628090807298
gradient difference: 0.018309924792055236
At round 198 accuracy: 0.7068322981366459
At round 198 training accuracy: 0.7345818079414249
At round 198 training loss: 0.7843016299755599
gradient difference: 0.0182754646568345
At round 199 accuracy: 0.7105590062111802
At round 199 training accuracy: 0.7348634187552802
At round 199 training loss: 0.7833710933144548
gradient difference: 0.018247975963462636
At round 200 accuracy: 0.7105590062111802
At round 200 training accuracy: 0.7361306674176289
