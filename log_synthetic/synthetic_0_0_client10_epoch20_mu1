Arguments:
	       batch_size : 10
	clients_per_round : 10
	          dataset : synthetic_0_0
	     drop_percent : 0.0
	       eval_every : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (10,)
	               mu : 1.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 200
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/5.44k flops)
  dense/kernel/Regularizer/l2_regularizer (1/1.80k flops)
    dense/kernel/Regularizer/l2_regularizer/L2Loss (1.80k/1.80k flops)
  dense/kernel/Initializer/random_uniform (600/1.20k flops)
    dense/kernel/Initializer/random_uniform/mul (600/600 flops)
    dense/kernel/Initializer/random_uniform/sub (1/1 flops)
  PGD/update_dense/kernel/AssignSub (600/600 flops)
  PGD/update_dense/kernel/mul (600/600 flops)
  PGD/update_dense/kernel/mul_1 (600/600 flops)
  PGD/update_dense/kernel/sub (600/600 flops)
  PGD/update_dense/bias/AssignSub (10/10 flops)
  PGD/update_dense/bias/mul (10/10 flops)
  PGD/update_dense/bias/mul_1 (10/10 flops)
  PGD/update_dense/bias/sub (10/10 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/Neg (1/1 flops)
  gradients/sparse_softmax_cross_entropy_loss/value_grad/mul (1/1 flops)
  sparse_softmax_cross_entropy_loss/num_present/Equal (1/1 flops)

======================End of Report==========================
30 Clients in Total
Training with 10 workers ---
At round 0 accuracy: 0.03455964325529543
At round 0 training accuracy: 0.03406510219530658
At round 0 training loss: 2.8860849624190883
gradient difference: 48.50957886741199
At round 1 accuracy: 0.08361204013377926
At round 1 training accuracy: 0.08516275548826646
At round 1 training loss: 2.6479423216368074
gradient difference: 46.602446925557764
At round 2 accuracy: 0.2709030100334448
At round 2 training accuracy: 0.28274034822104466
At round 2 training loss: 2.0425677710851273
gradient difference: 42.17505465742209
At round 3 accuracy: 0.3311036789297659
At round 3 training accuracy: 0.3374968458238708
At round 3 training loss: 1.824178888254505
gradient difference: 39.71309883453956
At round 4 accuracy: 0.5027870680044593
At round 4 training accuracy: 0.5141307090587939
At round 4 training loss: 1.5382188827404764
gradient difference: 36.92856396443782
At round 5 accuracy: 0.5797101449275363
At round 5 training accuracy: 0.5826394145849104
At round 5 training loss: 1.4171673775437412
gradient difference: 35.73182332507223
At round 6 accuracy: 0.5719063545150501
At round 6 training accuracy: 0.5666161998485996
At round 6 training loss: 1.373568256101974
gradient difference: 35.08168686729787
At round 7 accuracy: 0.5919732441471572
At round 7 training accuracy: 0.5951299520565229
At round 7 training loss: 1.2565379032177544
gradient difference: 33.877679484218376
At round 8 accuracy: 0.6086956521739131
At round 8 training accuracy: 0.6077466565733031
At round 8 training loss: 1.2114789587547314
gradient difference: 33.50615347149007
At round 9 accuracy: 0.6276477146042363
At round 9 training accuracy: 0.6332323996971991
At round 9 training loss: 1.152617961264171
gradient difference: 32.32791505591354
At round 10 accuracy: 0.6276477146042363
At round 10 training accuracy: 0.6338632349230381
At round 10 training loss: 1.1280293453248829
gradient difference: 32.16028418133563
At round 11 accuracy: 0.6287625418060201
At round 11 training accuracy: 0.6375220792329044
At round 11 training loss: 1.1194926418003082
gradient difference: 32.18214774935303
At round 12 accuracy: 0.6321070234113713
At round 12 training accuracy: 0.6382790815039112
At round 12 training loss: 1.0943745563806435
gradient difference: 31.926470764730436
At round 13 accuracy: 0.6287625418060201
At round 13 training accuracy: 0.6362604087812264
At round 13 training loss: 1.0748982782325953
gradient difference: 31.59826711217185
At round 14 accuracy: 0.6276477146042363
At round 14 training accuracy: 0.6303305576583397
At round 14 training loss: 1.0528837911654927
gradient difference: 31.257331846500996
At round 15 accuracy: 0.6343366778149386
At round 15 training accuracy: 0.6404239212717638
At round 15 training loss: 1.0387541797327466
gradient difference: 30.998539786755863
At round 16 accuracy: 0.6365663322185061
At round 16 training accuracy: 0.6450921019429725
At round 16 training loss: 1.0129550436587778
gradient difference: 30.724787061774915
At round 17 accuracy: 0.6399108138238573
At round 17 training accuracy: 0.6507696189755235
At round 17 training loss: 0.9920172075798099
gradient difference: 30.41678773855031
At round 18 accuracy: 0.6421404682274248
At round 18 training accuracy: 0.652409790562705
At round 18 training loss: 0.9849061727018075
gradient difference: 30.458393448215123
At round 19 accuracy: 0.6365663322185061
At round 19 training accuracy: 0.6525359576078729
At round 19 training loss: 0.9720739633582369
gradient difference: 29.88153268321502
At round 20 accuracy: 0.6365663322185061
At round 20 training accuracy: 0.6531667928337118
At round 20 training loss: 0.9483070877123809
gradient difference: 28.75300880721856
At round 21 accuracy: 0.6432552954292085
At round 21 training accuracy: 0.6577088064597527
At round 21 training loss: 0.9321130617208442
gradient difference: 28.034779328294878
At round 22 accuracy: 0.6454849498327759
At round 22 training accuracy: 0.6590966439565985
At round 22 training loss: 0.9273558394476271
gradient difference: 28.131132242033914
At round 23 accuracy: 0.6465997770345596
At round 23 training accuracy: 0.6594751450921019
At round 23 training loss: 0.9220077565603509
gradient difference: 28.134036351841935
At round 24 accuracy: 0.6477146042363434
At round 24 training accuracy: 0.6656573303053243
At round 24 training loss: 0.9036605686735181
gradient difference: 27.42780540268603
At round 25 accuracy: 0.6488294314381271
At round 25 training accuracy: 0.6664143325763311
At round 25 training loss: 0.9016240119688267
gradient difference: 27.491355448048775
At round 26 accuracy: 0.6588628762541806
At round 26 training accuracy: 0.6700731768861973
At round 26 training loss: 0.8931686092791324
gradient difference: 27.451927818695097
At round 27 accuracy: 0.6566332218506131
At round 27 training accuracy: 0.6685591723441837
At round 27 training loss: 0.8888205141220369
gradient difference: 27.325197169317253
At round 28 accuracy: 0.6510590858416946
At round 28 training accuracy: 0.6698208427958617
At round 28 training loss: 0.8807960860874178
gradient difference: 27.067524664479254
At round 29 accuracy: 0.6544035674470458
At round 29 training accuracy: 0.672848851879889
At round 29 training loss: 0.8721362035484186
gradient difference: 26.539988971609898
At round 30 accuracy: 0.6622073578595318
At round 30 training accuracy: 0.6749936916477416
At round 30 training loss: 0.8565863603822381
gradient difference: 25.999304780793448
At round 31 accuracy: 0.6700111482720178
At round 31 training accuracy: 0.6792833711834468
At round 31 training loss: 0.8396012441351739
gradient difference: 25.227633864192196
At round 32 accuracy: 0.6711259754738016
At round 32 training accuracy: 0.6811758768609639
At round 32 training loss: 0.835705703048615
gradient difference: 25.072601847909716
At round 33 accuracy: 0.673355629877369
At round 33 training accuracy: 0.684960888215998
At round 33 training loss: 0.8325496181272725
gradient difference: 24.891132158580074
At round 34 accuracy: 0.6800445930880713
At round 34 training accuracy: 0.6859702245773404
At round 34 training loss: 0.8273295651949282
gradient difference: 24.7298878070907
At round 35 accuracy: 0.6778149386845039
At round 35 training accuracy: 0.6922785768357305
At round 35 training loss: 0.817016865865525
gradient difference: 24.43873025419215
At round 36 accuracy: 0.6889632107023411
At round 36 training accuracy: 0.6971990915972748
At round 36 training loss: 0.8040320878000538
gradient difference: 24.229214834186966
At round 37 accuracy: 0.6833890746934225
At round 37 training accuracy: 0.6974514256876104
At round 37 training loss: 0.7970075025623878
gradient difference: 23.806126888460028
At round 38 accuracy: 0.68561872909699
At round 38 training accuracy: 0.7006056018168054
At round 38 training loss: 0.7932955347594469
gradient difference: 23.56345473187845
At round 39 accuracy: 0.6889632107023411
At round 39 training accuracy: 0.700227100681302
At round 39 training loss: 0.7901092718405649
gradient difference: 23.461016148098512
At round 40 accuracy: 0.6867335562987736
At round 40 training accuracy: 0.7012364370426445
At round 40 training loss: 0.7846717756828366
gradient difference: 23.033143623965863
At round 41 accuracy: 0.6923076923076923
At round 41 training accuracy: 0.7035074438556649
At round 41 training loss: 0.7805498303004679
gradient difference: 23.01230628406779
At round 42 accuracy: 0.6923076923076923
At round 42 training accuracy: 0.7035074438556649
At round 42 training loss: 0.7787889653616831
gradient difference: 22.82166444075689
At round 43 accuracy: 0.6945373467112598
At round 43 training accuracy: 0.7026242745394903
At round 43 training loss: 0.775220120850332
gradient difference: 22.53829576395972
At round 44 accuracy: 0.7001114827201784
At round 44 training accuracy: 0.7146101438304315
At round 44 training loss: 0.7596008156405335
gradient difference: 21.435810056968474
At round 45 accuracy: 0.7045707915273133
At round 45 training accuracy: 0.714988644965935
At round 45 training loss: 0.7543203664787063
gradient difference: 20.704279671888173
At round 46 accuracy: 0.7023411371237458
At round 46 training accuracy: 0.719404491546808
At round 46 training loss: 0.746613696592369
gradient difference: 20.641342987468636
At round 47 accuracy: 0.7101449275362319
At round 47 training accuracy: 0.7279838506182186
At round 47 training loss: 0.7389409793950918
gradient difference: 19.83449935228928
At round 48 accuracy: 0.7101449275362319
At round 48 training accuracy: 0.7276053494827152
At round 48 training loss: 0.7375491160780488
gradient difference: 19.715468546260585
At round 49 accuracy: 0.7134894091415831
At round 49 training accuracy: 0.7269745142568761
At round 49 training loss: 0.7366522199771791
gradient difference: 19.607878551313785
At round 50 accuracy: 0.7146042363433668
At round 50 training accuracy: 0.7268483472117083
At round 50 training loss: 0.7305087219988831
gradient difference: 19.541723465401976
At round 51 accuracy: 0.717948717948718
At round 51 training accuracy: 0.7311380267474136
At round 51 training loss: 0.725280958082698
gradient difference: 19.588819915480226
At round 52 accuracy: 0.7224080267558528
At round 52 training accuracy: 0.7354277062831188
At round 52 training loss: 0.7230290696926137
gradient difference: 19.45357717271638
At round 53 accuracy: 0.7279821627647715
At round 53 training accuracy: 0.7360585415089579
At round 53 training loss: 0.7157073147177091
gradient difference: 18.69586336432646
At round 54 accuracy: 0.7290969899665551
At round 54 training accuracy: 0.7385818824123139
At round 54 training loss: 0.7126839118551415
gradient difference: 18.328512595650704
At round 55 accuracy: 0.7268673355629878
At round 55 training accuracy: 0.7358062074186222
At round 55 training loss: 0.704090938621856
gradient difference: 17.859817908595502
At round 56 accuracy: 0.7313266443701226
At round 56 training accuracy: 0.739843552863992
At round 56 training loss: 0.7011154092776155
gradient difference: 17.998121082045078
At round 57 accuracy: 0.7402452619843924
At round 57 training accuracy: 0.7485490789805703
At round 57 training loss: 0.694571376943511
gradient difference: 17.877251882610675
At round 58 accuracy: 0.7447045707915273
At round 58 training accuracy: 0.7496845823870805
At round 58 training loss: 0.6930142527231357
gradient difference: 17.84971515679012
At round 59 accuracy: 0.7402452619843924
At round 59 training accuracy: 0.7498107494322483
At round 59 training loss: 0.6872103680594966
gradient difference: 17.268524214613155
At round 60 accuracy: 0.7435897435897436
At round 60 training accuracy: 0.7486752460257381
At round 60 training loss: 0.6824953968947133
gradient difference: 16.769422549363448
At round 61 accuracy: 0.7491638795986622
At round 61 training accuracy: 0.753848094877618
At round 61 training loss: 0.6797563328543772
gradient difference: 16.691392886239484
At round 62 accuracy: 0.745819397993311
At round 62 training accuracy: 0.7527125914711077
At round 62 training loss: 0.6789374510907323
gradient difference: 16.585738647992745
At round 63 accuracy: 0.7491638795986622
At round 63 training accuracy: 0.753217259651779
At round 63 training loss: 0.6766779289500745
gradient difference: 16.28564667060671
At round 64 accuracy: 0.7525083612040134
At round 64 training accuracy: 0.7606611153166792
At round 64 training loss: 0.6666007219502853
gradient difference: 15.699073387716156
At round 65 accuracy: 0.750278706800446
At round 65 training accuracy: 0.7606611153166792
At round 65 training loss: 0.6658693589324166
gradient difference: 15.59875287915464
At round 66 accuracy: 0.7625418060200669
At round 66 training accuracy: 0.7638152914458743
At round 66 training loss: 0.6644408979600964
gradient difference: 15.781340967714538
At round 67 accuracy: 0.7603121516164995
At round 67 training accuracy: 0.7643199596265455
At round 67 training loss: 0.661721847543797
gradient difference: 15.522170777384416
At round 68 accuracy: 0.7670011148272018
At round 68 training accuracy: 0.7688619732525864
At round 68 training loss: 0.6598356022020563
gradient difference: 15.552961362783808
At round 69 accuracy: 0.7647714604236343
At round 69 training accuracy: 0.768483472117083
At round 69 training loss: 0.6592601079861778
gradient difference: 15.62097311117992
At round 70 accuracy: 0.7647714604236343
At round 70 training accuracy: 0.768483472117083
At round 70 training loss: 0.6587697389722904
gradient difference: 15.574937550283625
At round 71 accuracy: 0.7614269788182831
At round 71 training accuracy: 0.769745142568761
At round 71 training loss: 0.6587073263330815
gradient difference: 15.70422762405049
At round 72 accuracy: 0.7670011148272018
At round 72 training accuracy: 0.7706283118849356
At round 72 training loss: 0.6571677515255967
gradient difference: 15.457107060875817
At round 73 accuracy: 0.770345596432553
At round 73 training accuracy: 0.7708806459752713
At round 73 training loss: 0.6503727947042867
gradient difference: 15.201786450626733
At round 74 accuracy: 0.7670011148272018
At round 74 training accuracy: 0.7739086550592985
At round 74 training loss: 0.6462631873259335
gradient difference: 14.903833745264276
At round 75 accuracy: 0.7647714604236343
At round 75 training accuracy: 0.7739086550592985
At round 75 training loss: 0.6465585485749124
gradient difference: 14.930292296891954
At round 76 accuracy: 0.7625418060200669
At round 76 training accuracy: 0.7725208175624527
At round 76 training loss: 0.6451175525231527
gradient difference: 14.81275292144208
At round 77 accuracy: 0.7670011148272018
At round 77 training accuracy: 0.7741609891496342
At round 77 training loss: 0.6445620699330681
gradient difference: 14.889412342572848
At round 78 accuracy: 0.7692307692307693
At round 78 training accuracy: 0.7725208175624527
At round 78 training loss: 0.6410666536610294
gradient difference: 14.684383757252109
At round 79 accuracy: 0.7681159420289855
At round 79 training accuracy: 0.771006813020439
At round 79 training loss: 0.6358101350182929
gradient difference: 14.072320695312545
At round 80 accuracy: 0.7658862876254181
At round 80 training accuracy: 0.7718899823366137
At round 80 training loss: 0.6353567527955359
gradient difference: 14.152641568085219
At round 81 accuracy: 0.7658862876254181
At round 81 training accuracy: 0.7718899823366137
At round 81 training loss: 0.6348079474618903
gradient difference: 14.05557793294812
At round 82 accuracy: 0.7636566332218506
At round 82 training accuracy: 0.7702498107494322
At round 82 training loss: 0.6357664695860775
gradient difference: 14.126138124196794
At round 83 accuracy: 0.7647714604236343
At round 83 training accuracy: 0.7679788039364118
At round 83 training loss: 0.6360528806339383
gradient difference: 13.942203498343135
At round 84 accuracy: 0.7647714604236343
At round 84 training accuracy: 0.7679788039364118
At round 84 training loss: 0.6317898954949622
gradient difference: 13.281117410604644
At round 85 accuracy: 0.7692307692307693
At round 85 training accuracy: 0.7708806459752713
At round 85 training loss: 0.6312629005769721
gradient difference: 13.387732916287268
At round 86 accuracy: 0.7670011148272018
At round 86 training accuracy: 0.7689881402977542
At round 86 training loss: 0.6313600256954939
gradient difference: 13.317611898838539
At round 87 accuracy: 0.7636566332218506
At round 87 training accuracy: 0.7705021448397679
At round 87 training loss: 0.6303552772964716
gradient difference: 13.4067680882001
At round 88 accuracy: 0.7603121516164995
At round 88 training accuracy: 0.7681049709815796
At round 88 training loss: 0.6315360345257061
gradient difference: 13.287418699238694
At round 89 accuracy: 0.7658862876254181
At round 89 training accuracy: 0.7706283118849356
At round 89 training loss: 0.6285816474217022
gradient difference: 13.330623766173526
At round 90 accuracy: 0.7658862876254181
At round 90 training accuracy: 0.7687358062074187
At round 90 training loss: 0.6196661350411771
gradient difference: 12.619380307456943
At round 91 accuracy: 0.7681159420289855
At round 91 training accuracy: 0.7687358062074187
At round 91 training loss: 0.6172103835065355
gradient difference: 11.973736286920559
At round 92 accuracy: 0.7681159420289855
At round 92 training accuracy: 0.771006813020439
At round 92 training loss: 0.6138313824772448
gradient difference: 12.005471540106845
At round 93 accuracy: 0.770345596432553
At round 93 training accuracy: 0.7711329800656068
At round 93 training loss: 0.6119777649709658
gradient difference: 11.803562532388646
At round 94 accuracy: 0.7692307692307693
At round 94 training accuracy: 0.7689881402977542
At round 94 training loss: 0.6115508050298957
gradient difference: 11.597843117818996
At round 95 accuracy: 0.770345596432553
At round 95 training accuracy: 0.7702498107494322
At round 95 training loss: 0.6100818747765009
gradient difference: 11.52399106493284
At round 96 accuracy: 0.7681159420289855
At round 96 training accuracy: 0.768483472117083
At round 96 training loss: 0.6094240910986285
gradient difference: 11.492369376169906
At round 97 accuracy: 0.7692307692307693
At round 97 training accuracy: 0.7739086550592985
At round 97 training loss: 0.6056807897839279
gradient difference: 11.674350037400652
At round 98 accuracy: 0.770345596432553
At round 98 training accuracy: 0.7741609891496342
At round 98 training loss: 0.6039841692201904
gradient difference: 11.489059205956192
At round 99 accuracy: 0.770345596432553
At round 99 training accuracy: 0.7731516527882917
At round 99 training loss: 0.6038321443242844
gradient difference: 11.467545598540855
At round 100 accuracy: 0.7692307692307693
At round 100 training accuracy: 0.771006813020439
At round 100 training loss: 0.6037115962743955
gradient difference: 11.361124404371587
At round 101 accuracy: 0.7692307692307693
At round 101 training accuracy: 0.7706283118849356
At round 101 training loss: 0.6041393410933817
gradient difference: 11.412897748288525
At round 102 accuracy: 0.7692307692307693
At round 102 training accuracy: 0.7686096391622508
At round 102 training loss: 0.6039760738717913
gradient difference: 11.204395935048705
At round 103 accuracy: 0.7658862876254181
At round 103 training accuracy: 0.7686096391622508
At round 103 training loss: 0.6040288325371664
gradient difference: 11.179788434114235
At round 104 accuracy: 0.7647714604236343
At round 104 training accuracy: 0.7677264698460762
At round 104 training loss: 0.6035348008821079
gradient difference: 11.117877645939718
At round 105 accuracy: 0.7681159420289855
At round 105 training accuracy: 0.7692404743880898
At round 105 training loss: 0.6018614897322555
gradient difference: 11.231199634134727
At round 106 accuracy: 0.7658862876254181
At round 106 training accuracy: 0.7678526368912441
At round 106 training loss: 0.6017138099205289
gradient difference: 11.095231991786902
At round 107 accuracy: 0.770345596432553
At round 107 training accuracy: 0.7698713096139288
At round 107 training loss: 0.6004778171533217
gradient difference: 11.041046597865932
At round 108 accuracy: 0.7681159420289855
At round 108 training accuracy: 0.7721423164269493
At round 108 training loss: 0.5978420439707136
gradient difference: 11.083488427705621
At round 109 accuracy: 0.7681159420289855
At round 109 training accuracy: 0.7720161493817814
At round 109 training loss: 0.5972647357915596
gradient difference: 11.015349929712404
At round 110 accuracy: 0.7759197324414716
At round 110 training accuracy: 0.7737824880141307
At round 110 training loss: 0.5906280686161376
gradient difference: 10.097748711942392
At round 111 accuracy: 0.7748049052396878
At round 111 training accuracy: 0.7732778198334594
At round 111 training loss: 0.591159321224388
gradient difference: 10.08590287759286
At round 112 accuracy: 0.7759197324414716
At round 112 training accuracy: 0.7747918243754731
At round 112 training loss: 0.591253819337024
gradient difference: 10.079963737628141
At round 113 accuracy: 0.7814938684503902
At round 113 training accuracy: 0.7827403482210447
At round 113 training loss: 0.5886062544896501
gradient difference: 10.348993946231301
At round 114 accuracy: 0.7792642140468228
At round 114 training accuracy: 0.785011355034065
At round 114 training loss: 0.589668938249595
gradient difference: 10.484113367622296
At round 115 accuracy: 0.7759197324414716
At round 115 training accuracy: 0.7778198334595003
At round 115 training loss: 0.5908791678935316
gradient difference: 10.313386209739711
At round 116 accuracy: 0.778149386845039
At round 116 training accuracy: 0.7811001766338632
At round 116 training loss: 0.5873394685455697
gradient difference: 10.10255225405549
At round 117 accuracy: 0.7814938684503902
At round 117 training accuracy: 0.7833711834468837
At round 117 training loss: 0.5856475788142759
gradient difference: 10.175421950302178
At round 118 accuracy: 0.7848383500557413
At round 118 training accuracy: 0.7817310118597023
At round 118 training loss: 0.5853772579955274
gradient difference: 10.018481001367862
At round 119 accuracy: 0.7792642140468228
At round 119 training accuracy: 0.780595508453192
At round 119 training loss: 0.5842415608869644
gradient difference: 9.877503702373215
At round 120 accuracy: 0.7770345596432553
At round 120 training accuracy: 0.7851375220792329
At round 120 training loss: 0.5840251609252274
gradient difference: 10.05341665890967
At round 121 accuracy: 0.7770345596432553
At round 121 training accuracy: 0.7881655311632602
At round 121 training loss: 0.5804334352452823
gradient difference: 9.944012297397721
At round 122 accuracy: 0.7837235228539576
At round 122 training accuracy: 0.7874085288922533
At round 122 training loss: 0.578555290036631
gradient difference: 9.562135993447477
At round 123 accuracy: 0.782608695652174
At round 123 training accuracy: 0.7885440322987636
At round 123 training loss: 0.579595694941528
gradient difference: 9.691450401831304
At round 124 accuracy: 0.782608695652174
At round 124 training accuracy: 0.7885440322987636
At round 124 training loss: 0.5772709157837237
gradient difference: 9.629406817809565
At round 125 accuracy: 0.7848383500557413
At round 125 training accuracy: 0.7890487004794348
At round 125 training loss: 0.5758004445608949
gradient difference: 9.591938195489066
At round 126 accuracy: 0.7837235228539576
At round 126 training accuracy: 0.7898057027504416
At round 126 training loss: 0.5734447867226151
gradient difference: 9.362000557604286
At round 127 accuracy: 0.7803790412486065
At round 127 training accuracy: 0.7880393641180924
At round 127 training loss: 0.5740239651830937
gradient difference: 9.33858229209183
At round 128 accuracy: 0.7803790412486065
At round 128 training accuracy: 0.7871561948019178
At round 128 training loss: 0.5737045153894058
gradient difference: 9.266195067389845
At round 129 accuracy: 0.782608695652174
At round 129 training accuracy: 0.7866515266212465
At round 129 training loss: 0.5727842221305164
gradient difference: 9.037970402884246
At round 130 accuracy: 0.7814938684503902
At round 130 training accuracy: 0.7891748675246025
At round 130 training loss: 0.5706333002622408
gradient difference: 9.113273119978317
At round 131 accuracy: 0.7848383500557413
At round 131 training accuracy: 0.7885440322987636
At round 131 training loss: 0.5690713443044515
gradient difference: 8.891803935209788
At round 132 accuracy: 0.782608695652174
At round 132 training accuracy: 0.7881655311632602
At round 132 training loss: 0.5680014594517264
gradient difference: 8.777519065318879
At round 133 accuracy: 0.782608695652174
At round 133 training accuracy: 0.786399192530911
At round 133 training loss: 0.5672493381280623
gradient difference: 8.651737811413161
At round 134 accuracy: 0.782608695652174
At round 134 training accuracy: 0.7866515266212465
At round 134 training loss: 0.566866626958848
gradient difference: 8.604721588695332
At round 135 accuracy: 0.7881828316610925
At round 135 training accuracy: 0.7932122129699722
At round 135 training loss: 0.5654907610803563
gradient difference: 8.763353073411322
At round 136 accuracy: 0.79041248606466
At round 136 training accuracy: 0.7925813777441333
At round 136 training loss: 0.5626830873009097
gradient difference: 8.596494131241739
At round 137 accuracy: 0.7915273132664437
At round 137 training accuracy: 0.7939692152409791
At round 137 training loss: 0.5623345245890847
gradient difference: 8.643198948586424
At round 138 accuracy: 0.79041248606466
At round 138 training accuracy: 0.7961140550088317
At round 138 training loss: 0.5612282385789243
gradient difference: 8.510855151641335
At round 139 accuracy: 0.7915273132664437
At round 139 training accuracy: 0.795861720918496
At round 139 training loss: 0.5606427501233535
gradient difference: 8.537656434162253
At round 140 accuracy: 0.7892976588628763
At round 140 training accuracy: 0.7957355538733283
At round 140 training loss: 0.561169007297562
gradient difference: 8.43527324705055
At round 141 accuracy: 0.7892976588628763
At round 141 training accuracy: 0.7949785516023214
At round 141 training loss: 0.5612055694112386
gradient difference: 8.197816496187878
At round 142 accuracy: 0.7915273132664437
At round 142 training accuracy: 0.7940953822861468
At round 142 training loss: 0.5600583798912644
gradient difference: 7.933412217117311
At round 143 accuracy: 0.7915273132664437
At round 143 training accuracy: 0.7937168811506434
At round 143 training loss: 0.5580256877665263
gradient difference: 7.620981222382424
At round 144 accuracy: 0.79041248606466
At round 144 training accuracy: 0.791445874337623
At round 144 training loss: 0.5594134884298932
gradient difference: 7.467902210661072
At round 145 accuracy: 0.7937569676700111
At round 145 training accuracy: 0.7964925561443351
At round 145 training loss: 0.5550925734149983
gradient difference: 7.525776025812186
At round 146 accuracy: 0.8004459308807135
At round 146 training accuracy: 0.7976280595508454
At round 146 training loss: 0.5550810753690032
gradient difference: 7.584625850826845
At round 147 accuracy: 0.7993311036789298
At round 147 training accuracy: 0.7971233913701741
At round 147 training loss: 0.5561466297264479
gradient difference: 7.677697114624423
At round 148 accuracy: 0.7993311036789298
At round 148 training accuracy: 0.7957355538733283
At round 148 training loss: 0.5547393237336025
gradient difference: 7.67569397197621
At round 149 accuracy: 0.7948717948717948
At round 149 training accuracy: 0.7951047186474893
At round 149 training loss: 0.5500365562566375
gradient difference: 7.136242004990432
At round 150 accuracy: 0.7948717948717948
At round 150 training accuracy: 0.7948523845571537
At round 150 training loss: 0.5468612150987081
gradient difference: 6.980372718069558
At round 151 accuracy: 0.7948717948717948
At round 151 training accuracy: 0.7924552106989654
At round 151 training loss: 0.5474566902153132
gradient difference: 6.951105655853113
At round 152 accuracy: 0.7971014492753623
At round 152 training accuracy: 0.7937168811506434
At round 152 training loss: 0.5472688314607383
gradient difference: 7.0046404339117005
At round 153 accuracy: 0.7959866220735786
At round 153 training accuracy: 0.7935907141054757
At round 153 training loss: 0.5464718152113013
gradient difference: 6.949972585835032
At round 154 accuracy: 0.7959866220735786
At round 154 training accuracy: 0.7952308856926571
At round 154 training loss: 0.5466944496264837
gradient difference: 7.010552993119399
At round 155 accuracy: 0.7959866220735786
At round 155 training accuracy: 0.794600050466818
At round 155 training loss: 0.5453885638048351
gradient difference: 6.927011240800957
At round 156 accuracy: 0.7948717948717948
At round 156 training accuracy: 0.7934645470603079
At round 156 training loss: 0.5444557988661789
gradient difference: 6.819814694210183
At round 157 accuracy: 0.79041248606466
At round 157 training accuracy: 0.7940953822861468
At round 157 training loss: 0.5443158747966502
gradient difference: 6.658303609375056
At round 158 accuracy: 0.7937569676700111
At round 158 training accuracy: 0.7937168811506434
At round 158 training loss: 0.5430677311377209
gradient difference: 6.495292483429168
At round 159 accuracy: 0.7926421404682275
At round 159 training accuracy: 0.79333838001514
At round 159 training loss: 0.5426010128056361
gradient difference: 6.507190431325333
At round 160 accuracy: 0.7915273132664437
At round 160 training accuracy: 0.7911935402472874
At round 160 training loss: 0.5432208897354405
gradient difference: 6.597193304071874
At round 161 accuracy: 0.7915273132664437
At round 161 training accuracy: 0.7918243754731265
At round 161 training loss: 0.5433966165616236
gradient difference: 6.565314213331934
At round 162 accuracy: 0.7892976588628763
At round 162 training accuracy: 0.790184203885945
At round 162 training loss: 0.5448322127064525
gradient difference: 6.56719286847567
At round 163 accuracy: 0.8015607580824972
At round 163 training accuracy: 0.7975018925056775
At round 163 training loss: 0.5401504526409238
gradient difference: 6.496468663218232
At round 164 accuracy: 0.8004459308807135
At round 164 training accuracy: 0.7991420640928589
At round 164 training loss: 0.5375222603838873
gradient difference: 6.205251103680053
At round 165 accuracy: 0.7993311036789298
At round 165 training accuracy: 0.7993943981831946
At round 165 training loss: 0.5369963722487753
gradient difference: 6.180141592750298
At round 166 accuracy: 0.8004459308807135
At round 166 training accuracy: 0.8005299015897047
At round 166 training loss: 0.536357347549386
gradient difference: 6.293075951948259
At round 167 accuracy: 0.8015607580824972
At round 167 training accuracy: 0.8000252334090335
At round 167 training loss: 0.5361318678765269
gradient difference: 6.29569003325103
At round 168 accuracy: 0.7993311036789298
At round 168 training accuracy: 0.8012869038607116
At round 168 training loss: 0.5360005347466145
gradient difference: 6.3842105871431665
At round 169 accuracy: 0.798216276477146
At round 169 training accuracy: 0.8020439061317184
At round 169 training loss: 0.5339377194059269
gradient difference: 6.286187500144966
At round 170 accuracy: 0.7993311036789298
At round 170 training accuracy: 0.8015392379510472
At round 170 training loss: 0.5336398533434983
gradient difference: 6.276380914637062
At round 171 accuracy: 0.798216276477146
At round 171 training accuracy: 0.8007822356800404
At round 171 training loss: 0.532841347546535
gradient difference: 6.213178454886017
At round 172 accuracy: 0.8015607580824972
At round 172 training accuracy: 0.8005299015897047
At round 172 training loss: 0.5326735962895757
gradient difference: 6.22795704307725
At round 173 accuracy: 0.802675585284281
At round 173 training accuracy: 0.8017915720413827
At round 173 training loss: 0.5325141275620476
gradient difference: 6.188221378429183
At round 174 accuracy: 0.8004459308807135
At round 174 training accuracy: 0.8017915720413827
At round 174 training loss: 0.5325468755992815
gradient difference: 6.14023851462317
At round 175 accuracy: 0.7971014492753623
At round 175 training accuracy: 0.8024224072672218
At round 175 training loss: 0.5330799738596866
gradient difference: 6.23789945978803
At round 176 accuracy: 0.7959866220735786
At round 176 training accuracy: 0.8017915720413827
At round 176 training loss: 0.5319625655228637
gradient difference: 6.17281412178143
At round 177 accuracy: 0.8049052396878483
At round 177 training accuracy: 0.8058289174867524
At round 177 training loss: 0.5312132540308391
gradient difference: 6.14901091176552
At round 178 accuracy: 0.8049052396878483
At round 178 training accuracy: 0.8049457481705778
At round 178 training loss: 0.531867771518964
gradient difference: 6.199782341910697
At round 179 accuracy: 0.802675585284281
At round 179 training accuracy: 0.802927075447893
At round 179 training loss: 0.532229692608432
gradient difference: 6.20335876667869
At round 180 accuracy: 0.8015607580824972
At round 180 training accuracy: 0.802927075447893
At round 180 training loss: 0.5320925846666642
gradient difference: 6.137747725214337
At round 181 accuracy: 0.8015607580824972
At round 181 training accuracy: 0.804188745899571
At round 181 training loss: 0.5309806989943348
gradient difference: 5.911846763277086
At round 182 accuracy: 0.8015607580824972
At round 182 training accuracy: 0.8036840777188998
At round 182 training loss: 0.531576252535621
gradient difference: 5.942885600087253
At round 183 accuracy: 0.8049052396878483
At round 183 training accuracy: 0.8051980822609135
At round 183 training loss: 0.5305615001472774
gradient difference: 5.7076173607288005
At round 184 accuracy: 0.8060200668896321
At round 184 training accuracy: 0.8060812515770881
At round 184 training loss: 0.5288083309618136
gradient difference: 5.543375839824625
At round 185 accuracy: 0.8071348940914158
At round 185 training accuracy: 0.8078475902094373
At round 185 training loss: 0.5262432846698017
gradient difference: 5.353812791605961
At round 186 accuracy: 0.8037904124860646
At round 186 training accuracy: 0.8075952561191017
At round 186 training loss: 0.5254208548956838
gradient difference: 4.831085796941323
At round 187 accuracy: 0.8015607580824972
At round 187 training accuracy: 0.8088569265707797
At round 187 training loss: 0.5240951826472334
gradient difference: 4.755318892458784
At round 188 accuracy: 0.802675585284281
At round 188 training accuracy: 0.8094877617966187
At round 188 training loss: 0.5242286075260034
gradient difference: 4.663267835128133
At round 189 accuracy: 0.8049052396878483
At round 189 training accuracy: 0.8086045924804441
At round 189 training loss: 0.5238251695927602
gradient difference: 4.634043579282136
At round 190 accuracy: 0.8037904124860646
At round 190 training accuracy: 0.808099924299773
At round 190 training loss: 0.5237111243991115
gradient difference: 4.6024880368023835
At round 191 accuracy: 0.8049052396878483
At round 191 training accuracy: 0.8083522583901085
At round 191 training loss: 0.5226241901917362
gradient difference: 4.7235051594343185
At round 192 accuracy: 0.8037904124860646
At round 192 training accuracy: 0.809361594751451
At round 192 training loss: 0.5222943749398871
gradient difference: 4.672029979279382
At round 193 accuracy: 0.8037904124860646
At round 193 training accuracy: 0.8096139288417865
At round 193 training loss: 0.521499573215847
gradient difference: 4.7276264937218055
At round 194 accuracy: 0.8071348940914158
At round 194 training accuracy: 0.8099924299772899
At round 194 training loss: 0.5210064714246905
gradient difference: 4.8397059224568535
At round 195 accuracy: 0.8082497212931996
At round 195 training accuracy: 0.8101185970224577
At round 195 training loss: 0.5204647840931307
gradient difference: 4.821917838427565
At round 196 accuracy: 0.8082497212931996
At round 196 training accuracy: 0.8099924299772899
At round 196 training loss: 0.5205062950109166
gradient difference: 4.805790235247552
At round 197 accuracy: 0.8115942028985508
At round 197 training accuracy: 0.8101185970224577
At round 197 training loss: 0.5202160555967573
gradient difference: 4.899523302578564
At round 198 accuracy: 0.8115942028985508
At round 198 training accuracy: 0.8103709311127933
At round 198 training loss: 0.5188216277712018
gradient difference: 4.814512898894899
At round 199 accuracy: 0.8082497212931996
At round 199 training accuracy: 0.8097400958869543
At round 199 training loss: 0.5187668893446898
gradient difference: 4.7916793103570345
At round 200 accuracy: 0.8127090301003345
At round 200 training accuracy: 0.8108755992934645
